# ğŸ­ Plant Analytics Platform

A production-ready, scalable data analytics platform for EPA eGRID (Emissions & Generation Resource Integrated Database) power plant data. Built with microservices architecture, automated data processing, and comprehensive monitoring.

> **âš¡ One-Command Setup**: Get the entire system running with just `make up` - no complex configuration needed!

## ğŸ“‹ Project Agenda

### **Primary Objectives**
- **Data Processing**: Automated ingestion and processing of EPA eGRID CSV datasets using Airflow
- **Analytics API**: RESTful APIs for power plant analytics and reporting
- **Authentication**: Secure JWT-based authentication  
- **Real-time Processing**: Queue-based data processing with Redis caching
- **Monitoring**: Production-ready observability with Prometheus and health checks

### **Key Features**
- âœ… **Infrastructure as Code (IaC)**: Helm charts for Kubernetes deployment
- âœ… **Microservices Architecture**: Independent scaling and health checks
- âœ… **NestJS Framework**: API Gateway and Auth Service converted to NestJS
- âœ… **Airflow ETL Pipeline**: Automated CSV data processing with hourly scheduling
- âœ… **JWT Authentication**: Secure token-based authentication service
- âœ… **Redis Caching**: Improved performance with intelligent caching
- âœ… **RabbitMQ Messaging**: Reliable asynchronous message processing
- âœ… **MinIO Object Storage**: Scalable file management system
- âœ… **Prometheus Monitoring**: Production-ready observability
- âœ… **Docker Containerization**: Smart dependency management
- âœ… **Secure Backend API**: Only accessible via API Gateway
- âœ… **Large-scale Data Support**: NUMERIC(20,2) for billions+ values
- âœ… **Production-ready**: Rate limiting, error handling, health checks

## ğŸš€ Setup Instructions

### **Prerequisites**
- Docker and Docker Compose
- Helm 3.0+ (for Kubernetes deployment)
- kubectl (for Kubernetes deployment)
- 3GB+ RAM, 5GB+ storage
- Make (available on macOS/Linux by default)

### **ğŸ³ Docker Compose Setup (Recommended for Development)**
```bash
# 1. Clone and navigate
git clone https://github.com/anshulchhabra1991/plant-analytics-platform.git
cd plant-analytics-platform

# 2. Start everything with one command
make up

# That's it! ğŸ‰ The system will:
# âœ… Start all services in the correct order
# âœ… Wait for databases to be ready with health checks
# âœ… Handle large-scale data (NUMERIC(20,2) for billions+ values)
# âœ… Secure backend API (only accessible via gateway)

### **â˜¸ï¸ Kubernetes Setup with Helm (Production Ready)**
```bash
# 1. Clone and navigate
git clone https://github.com/anshulchhabra1991/plant-analytics-platform.git
cd plant-analytics-platform

# 2. Install with Helm (Infrastructure as Code)
./helm/install.sh

# 3. Port-forward to access services
kubectl port-forward -n plant-analytics svc/plant-analytics-frontend 4000:4000
kubectl port-forward -n plant-analytics svc/plant-analytics-api-gateway 8000:8000

# 4. To uninstall
./helm/uninstall.sh
# âœ… Seed sample data automatically
# âœ… Show you what URLs to visit
```

### **Development Workflow**
```bash
# Daily development (fastest)
make start              # Quick restart with Docker Compose

# Troubleshooting
make clean && make up   # Clean restart with full cleanup

# Get help anytime
make                    # Shows all available commands
```

### **First Time Access**
After running `make up`, visit these URLs:

1. **ğŸŒ Frontend Dashboard**: http://localhost:4000
   - Email: `admin@plantanalytics.com` 
   - Password: `password123`

2. **ğŸ’¨ Airflow (Data Processing)**: http://localhost:8080
   - Username: `admin` / Password: `admin`
   - Trigger the `process_csv_data_pipeline` DAG to load sample data

3. **ğŸ“ MinIO Console (File Storage)**: http://localhost:9001  
   - Username: `egriduser` / Password: `egridpass123`

## â“ Troubleshooting

### **Common Issues**

#### **Services Won't Start**
```bash
make clean && make up       # Full cleanup and restart
make status                 # Check what's running
make logs                   # Check for errors
```

#### **Port Already in Use**
```bash
# Check what's using the port (example for port 4000)
lsof -i :4000
# Kill the process or change the port in docker-compose.yml
```

#### **Data Not Loading**
```bash
make airflow                # Open Airflow UI
# Manually trigger 'process_csv_data_pipeline'
make logs-airflow          # Check processing logs
# Database now supports large values (billions+ MWh) with NUMERIC(20,2)
```

#### **API Not Responding**
```bash
make health                # Check API health (via gateway)
make logs-gateway          # Check gateway logs
make logs-backend          # Check backend logs
# Note: Backend API (port 3000) only accessible through gateway (port 8000)
# Direct access to localhost:3000 is blocked for security
```

### **Getting Help**
- Run `make` to see all available commands
- Check service logs with `make logs-[service]`  
- Use `make status` to see which services are running
- All services have health checks - look for "healthy" status

### **Service Access**
| Service | URL | Credentials | Notes |
|---------|-----|-------------|--------|
| ğŸŒ Frontend | http://localhost:4000 | admin@plantanalytics.com / password123 | Main dashboard |
| ğŸšª API Gateway | http://localhost:8000 | JWT required | All API requests |
| ğŸ” Auth Service | http://localhost:5001 | - | Login endpoints |
| ğŸ’¨ Airflow | http://localhost:8080 | admin / admin | Data processing |
| ğŸ“ MinIO Console | http://localhost:9001 | egriduser / egridpass123 | File storage |
| ğŸ“Š Prometheus | http://localhost:9090 | - | Metrics monitoring |
| ğŸ° RabbitMQ | http://localhost:15672 | guest / guest | Message queue |
| ğŸ—„ï¸ PostgreSQL | localhost:5432 | plantuser / plantpassword123 | Database (NUMERIC(20,2) precision) |
| ğŸ”¥ Redis | localhost:6379 | - | Cache |

> **Security**: Backend API (port 3000) is only accessible through the API Gateway - no direct external access.  
> **Data Scale**: Database supports large values (billions+ MWh) with NUMERIC(20,2) precision for power generation data.


## ğŸ› ï¸ Development Commands

### **Essential Commands**
```bash
# Get help (shows all commands)
make                    # Display help menu

# System Management
make up                 # Complete system startup
make start              # Quick Docker Compose start
make stop               # Stop all services

# Cleanup
make clean              # Complete cleanup (containers, volumes, images)
```

### **Monitoring & Debugging**
```bash
# View logs
make logs               # All service logs
make logs-backend       # Backend API logs only
make logs-gateway       # API Gateway logs only
make logs-frontend      # Frontend logs only
make logs-airflow       # Airflow logs only

# System health
make status             # Service status summary
make health             # Check API health endpoints
make test-api           # Test API endpoints
make test-system        # Full system health check
```

### **Quick Access (macOS)**
```bash
# Open services in browser
make frontend           # Opens http://localhost:4000
make airflow            # Opens http://localhost:8080
make minio              # Opens http://localhost:9001
```

### **Docker Operations**
```bash
# Building
make build              # Build all Docker images
make rebuild            # Rebuild with --no-cache

# Manual Docker Compose
docker compose up -d    # Start services
docker compose down     # Stop services
docker compose ps       # Service status
docker compose logs -f  # Follow logs

## ğŸ¯ Why This Platform?

### **ğŸš€ Developer Experience First**
- **One-Command Setup**: `make up` starts everything
- **Smart Dependencies**: Services wait for each other automatically
- **Helpful Commands**: `make` shows all available options
- **Quick Access**: `make frontend` opens the dashboard instantly

### **ğŸ—ï¸ Production-Ready Architecture**
- **Security**: Backend API only accessible through API Gateway (no direct port exposure)
- **Scalability**: All services independently scalable with health checks
- **Data Handling**: Supports large-scale data (NUMERIC(20,2) for billions+ values)
- **Monitoring**: Built-in health checks, metrics, and dependency management
- **Reliability**: Automatic restarts and smart startup orchestration

### **ğŸ“Š Real Data Processing**
- **Sample Data Included**: Ready-to-use EPA eGRID dataset
- **Visual Workflows**: Airflow DAGs for data processing
- **Interactive Dashboard**: Filter and explore power plant data
- **APIs Ready**: RESTful endpoints for integration

```

## ğŸ—ï¸ Architecture Overview

### **Logical Architecture**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React         â”‚    â”‚   API Gateway    â”‚    â”‚   Backend API   â”‚
â”‚   Frontend      â”‚â”€â”€â”€â–¶â”‚   (Express.js)   â”‚â”€â”€â”€â–¶â”‚   (NestJS)      â”‚
â”‚   (Port 4000)   â”‚    â”‚   (Port 8000)    â”‚    â”‚   (Port 3000)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
                       â”‚   Auth Service   â”‚             â”‚
                       â”‚   (Express.js)   â”‚             â”‚
                       â”‚   (Port 5001)    â”‚             â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                                                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow       â”‚    â”‚   RabbitMQ       â”‚    â”‚   PostgreSQL    â”‚
â”‚   ETL Pipeline  â”‚â”€â”€â”€â–¶â”‚   Message Queue  â”‚â”€â”€â”€â–¶â”‚   Database      â”‚
â”‚   (Port 8080)   â”‚    â”‚   (Port 5672)    â”‚    â”‚   (Port 5432)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                              â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   MinIO          â”‚           â”‚
                        â”‚   Object Storage â”‚           â”‚
                        â”‚   (Port 9000)    â”‚           â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
                                                       â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
                        â”‚   Redis Cache    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚   (Port 6379)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   Prometheus     â”‚
                        â”‚   Monitoring     â”‚
                        â”‚   (Port 9090)    â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Deployment Architecture**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Load Balancer                            â”‚
â”‚                       (nginx/ALB)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Application Tier                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Frontend    â”‚  Gateway     â”‚  Auth        â”‚  Backend API        â”‚
â”‚  Cluster     â”‚  Cluster     â”‚  Cluster     â”‚  Cluster            â”‚
â”‚  (2+ pods)   â”‚  (2+ pods)   â”‚  (2+ pods)   â”‚  (2+ pods)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Data Tier                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PostgreSQL  â”‚  Redis       â”‚  RabbitMQ    â”‚  MinIO              â”‚
â”‚  Primary +   â”‚  Cluster     â”‚  Cluster     â”‚  Cluster            â”‚
â”‚  Replica     â”‚  (3+ nodes)  â”‚  (3+ nodes)  â”‚  (4+ nodes)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Processing Tier                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Airflow     â”‚  Workers     â”‚  Scheduler   â”‚  Monitoring         â”‚
â”‚  Webserver   â”‚  (Auto-scale)â”‚  (HA)        â”‚  (Prometheus)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Infrastructure Architecture**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Kubernetes Cluster                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Namespace: frontend                                            â”‚
â”‚  â”œâ”€â”€ Frontend Deployment (2 replicas)                          â”‚
â”‚  â”œâ”€â”€ Frontend Service (ClusterIP)                              â”‚
â”‚  â””â”€â”€ Frontend Ingress (nginx)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Namespace: api                                                 â”‚
â”‚  â”œâ”€â”€ Gateway Deployment (2 replicas)                           â”‚
â”‚  â”œâ”€â”€ Auth Service Deployment (2 replicas)                      â”‚
â”‚  â”œâ”€â”€ Backend API Deployment (2 replicas)                       â”‚
â”‚  â””â”€â”€ API Services (ClusterIP)                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Namespace: data                                                â”‚
â”‚  â”œâ”€â”€ PostgreSQL StatefulSet (Primary + Replica)                â”‚
â”‚  â”œâ”€â”€ Redis StatefulSet (3 node cluster)                        â”‚
â”‚  â”œâ”€â”€ RabbitMQ StatefulSet (3 node cluster)                     â”‚
â”‚  â””â”€â”€ MinIO StatefulSet (4 node cluster)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Namespace: processing                                          â”‚
â”‚  â”œâ”€â”€ Airflow Webserver Deployment                              â”‚
â”‚  â”œâ”€â”€ Airflow Scheduler Deployment                              â”‚
â”‚  â””â”€â”€ Airflow Workers Deployment (Auto-scaling)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Namespace: monitoring                                          â”‚
â”‚  â”œâ”€â”€ Prometheus StatefulSet                                    â”‚
â”‚  â”œâ”€â”€ Grafana Deployment                                        â”‚
â”‚  â””â”€â”€ AlertManager Deployment                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Handling Non-Functional Requirements

### **Scalability**
- **Horizontal Scaling**: All services are stateless and scale independently
- **Database Scaling**: PostgreSQL with read replicas and connection pooling
- **Cache Strategy**: Redis cluster with data partitioning
- **Load Balancing**: Kubernetes services with stateless routing

### **Performance**
- **Response Time**: < 200ms for cached queries, < 1s for database queries
- **Throughput**: 1000+ requests/second per service instance
- **Data Processing**: Parallel chunk processing for large CSV files
- **Optimization**: Database indexing, query optimization, connection pooling

### **Reliability**
- **High Availability**: 99.9% uptime with redundant instances
- **Fault Tolerance**: Health checks, circuit breakers, retry mechanisms
- **Data Backup**: Automated daily backups with point-in-time recovery
- **Disaster Recovery**: Multi-region deployment support

### **Security**
- **Authentication**: JWT tokens with 1-hour expiry
- **Authorization**: JWT token-based access control
- **Data Protection**: Encryption at rest and in transit
- **Network Security**: Service mesh with mTLS

### **Maintainability**
- **Clean Architecture**: Domain-driven design with clear separation
- **Documentation**: Comprehensive API docs and architecture diagrams
- **Testing**: Unit, integration, and E2E testing
- **CI/CD**: Automated pipelines with quality gates

## ğŸ“Š Monitoring & Observability

### **Prometheus Metrics**
The platform collects comprehensive metrics:

```yaml
# Application Metrics
- http_requests_total (counter)
- http_request_duration_seconds (histogram)
- database_connections_active (gauge)
- cache_hit_ratio (gauge)
- queue_messages_pending (gauge)

# Infrastructure Metrics  
- cpu_usage_percent (gauge)
- memory_usage_bytes (gauge)
- disk_usage_percent (gauge)
- network_bytes_total (counter)

# Business Metrics
- csv_files_processed_total (counter)
- data_records_ingested_total (counter)
- api_calls_by_endpoint (counter)
```

### **Monitoring Stack**
```
Applications â”€â”€â–¶ Prometheus â”€â”€â–¶ Grafana
     â”‚              â”‚              â”‚
     â”‚              â”‚              â””â”€â–¶ Dashboards
     â”‚              â”‚
     â”‚              â””â”€â–¶ AlertManager â”€â”€â–¶ Notifications
     â”‚
     â””â”€â–¶ Logs â”€â”€â–¶ ELK Stack â”€â”€â–¶ Kibana
```

### **Health Checks**
- **Service Health**: HTTP health endpoints on all services
- **Database Health**: Connection and query performance monitoring
- **Queue Health**: Message processing rates and queue depth
- **File System Health**: Disk usage and I/O performance

### **Alerting Rules**
- Service down for > 5 minutes
- Response time > 2 seconds for > 10 requests
- Error rate > 5% for > 5 minutes
- Database connections > 80% of pool
- Queue depth > 1000 messages

## ğŸ› ï¸ Technology Choices & Rationale

### **Frontend: React + TypeScript**
**Why?** Mature ecosystem, strong typing, component reusability, good developer experience

### **API Gateway: Express.js**
**Why?** Lightweight, minimal overhead, extensive middleware ecosystem, easy to configure

### **Authentication: Dedicated Service**
**Why?** Centralized auth logic, independent scaling, easier security auditing, reusable across services

### **Backend API: NestJS + TypeScript**
**Why?** Built-in dependency injection, decorator-based architecture, excellent TypeScript support, enterprise patterns

### **Data Processing: Apache Airflow**
**Why?** Visual workflow management, robust scheduling, extensive monitoring, Python ecosystem

### **Database: PostgreSQL**
**Why?** ACID compliance, advanced indexing, JSON support, proven reliability for analytics workloads  
**Scale:** NUMERIC(20,2) precision for handling large-scale power generation data (billions+ MWh)  
**Performance:** Optimized for analytics workloads with proper indexing and caching

### **Caching: Redis**
**Why?** In-memory performance, rich data structures, pub/sub capabilities, clustering support

### **Message Queue: RabbitMQ**
**Why?** Message durability, flexible routing, management interface, AMQP standard compliance

### **Object Storage: MinIO**
**Why?** S3-compatible API, self-hosted option, high performance, no vendor lock-in

### **Containerization: Docker**
**Why?** Environment consistency, easy deployment, resource isolation, extensive tooling

### **Orchestration: Kubernetes**
**Why?** Auto-scaling, service discovery, rolling updates, declarative configuration

### **Monitoring: Prometheus + Grafana**
**Why?** Time-series optimized, powerful query language, rich visualization, alerting capabilities

## ğŸ“š Documentation

Complete documentation is available in the `docs/` directory:

```
docs/
â”œâ”€â”€ api/                    # API Documentation
â”‚   â”œâ”€â”€ sequence-diagrams/  # Request flow diagrams
â”‚   â””â”€â”€ endpoints/          # Endpoint specifications
â”œâ”€â”€ architecture/           # Architecture Documentation  
â”‚   â”œâ”€â”€ overview/           # System overview
â”‚   â”œâ”€â”€ security/           # Security implementation
â”‚   â””â”€â”€ schemas/            # Database schemas
â””â”€â”€ deployment/             # Deployment Documentation
    â”œâ”€â”€ docker/             # Docker configurations
    â””â”€â”€ kubernetes/         # K8s manifests
```

## ğŸ¯ Quick Reference

### **Common Workflows**

#### **ğŸš€ First Time Setup**
```bash
git clone <repository>
cd plant-analytics-platform
make up                 # Start everything (handles dependencies automatically)
make frontend           # Open dashboard
# System now supports large-scale data and secure API access
```

#### **â˜€ï¸ Daily Development**
```bash
make start              # Quick start
make logs-backend       # Debug specific service
make clean && make up   # Clean restart if issues
```

#### **ğŸ› Troubleshooting**
```bash
make status             # What's running?
make health             # Are APIs responding?
make logs               # Check all logs
make clean && make up   # Full cleanup and restart
```

#### **ğŸ” Data Processing**
```bash
make airflow            # Open Airflow UI
# Trigger 'process_csv_data_pipeline' DAG
make logs-airflow       # Check processing logs
```

## ğŸ“ Project Structure

```
plant-analytics-platform/
â”œâ”€â”€ apps/                           # Application services
â”‚   â”œâ”€â”€ api-gateway/               # API Gateway (NestJS)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ controllers/       # NestJS controllers
â”‚   â”‚   â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”‚   â””â”€â”€ modules/           # Feature modules
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”œâ”€â”€ auth-service/              # Authentication service (NestJS)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ controllers/       # Auth endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ services/          # JWT & user management
â”‚   â”‚   â”‚   â””â”€â”€ types/             # TypeScript interfaces
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”œâ”€â”€ backend-api/               # Main backend (NestJS)
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ modules/           # Feature modules (analytics, ingestion)
â”‚   â”‚   â”‚   â”œâ”€â”€ infrastructure/    # External services (cache, database)
â”‚   â”‚   â”‚   â””â”€â”€ shared/            # Common utilities
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”œâ”€â”€ data-processing/           # Airflow DAGs & processors
â”‚   â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”‚   â”œâ”€â”€ application/       # CSV processors
â”‚   â”‚   â”‚   â”œâ”€â”€ config/            # ETL configuration (hourly scheduling)
â”‚   â”‚   â”‚   â”œâ”€â”€ domain/            # Data models
â”‚   â”‚   â”‚   â””â”€â”€ infrastructure/    # DB & MinIO clients
â”‚   â”‚   â””â”€â”€ airflow.cfg
â”‚   â””â”€â”€ frontend/                  # React frontend
â”‚       â”œâ”€â”€ src/                   # React components
â”‚       â””â”€â”€ package.json
â”œâ”€â”€ helm/                          # Infrastructure as Code (IaC)
â”‚   â”œâ”€â”€ plant-analytics-platform/  # Main Helm chart
â”‚   â”‚   â”œâ”€â”€ templates/             # Kubernetes templates
â”‚   â”‚   â”‚   â”œâ”€â”€ frontend-deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ api-gateway-deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ auth-service-deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”œâ”€â”€ Chart.yaml             # Chart metadata & dependencies
â”‚   â”‚   â””â”€â”€ values.yaml            # Configuration values
â”‚   â”œâ”€â”€ install.sh                 # Automated installation
â”‚   â””â”€â”€ uninstall.sh               # Clean uninstallation
â”œâ”€â”€ docker-compose.yml             # Multi-service orchestration
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ api/                       # API documentation
â”‚   â”œâ”€â”€ architecture/              # System design
â”‚   â””â”€â”€ deployment/                # Deployment guides
â”œâ”€â”€ infra/                         # Infrastructure configuration
â”‚   â”œâ”€â”€ db/                        # Database setup & migrations
â”‚   â”‚   â”œâ”€â”€ init/                  # SQL initialization scripts
â”‚   â”‚   â””â”€â”€ sample-data/           # CSV datasets
â”‚   â”œâ”€â”€ prometheus/                # Monitoring config
â”‚   â””â”€â”€ rabbitmq/                  # Message queue config
â”œâ”€â”€ scripts/                       # Utility scripts
â”‚   â”œâ”€â”€ seed/                      # Data seeding
â”‚   â”œâ”€â”€ tests/                     # Test scripts
â”‚   â””â”€â”€ run-system.sh              # System startup
â””â”€â”€ Makefile                       # Build automation
```

### **Essential URLs**
| Purpose | URL | Quick Access |
|---------|-----|--------------|
| ğŸ¯ **Main Dashboard** | http://localhost:4000 | `make frontend` |
| ğŸ’¨ **Data Processing** | http://localhost:8080 | `make airflow` |
| ğŸ“ **File Storage** | http://localhost:9001 | `make minio` |
| ğŸšª **API Gateway** | http://localhost:8000 | Direct curl/Postman |

### **Service Ports**
- ğŸŒ Frontend: 4000 (external)
- ğŸšª Gateway: 8000 (external)  
- ğŸ” Auth: 5001 (external)
- âš™ï¸ Backend: 3000 (internal only)
- ğŸ’¨ Airflow: 8080 (external)
- ğŸ“Š Prometheus: 9090 (external)
- ğŸ—„ï¸ PostgreSQL: 5432 (external)
- ğŸ”¥ Redis: 6379 (external)
- ğŸ° RabbitMQ: 5672 (external)
- ğŸ“ MinIO: 9000/9001 (external)

### **Alternative Commands**
```bash
# Direct Docker Compose usage (if make is not available)
docker compose up -d --build       # Complete startup with health checks
docker compose down -v             # Basic cleanup
docker compose ps                  # Check service status
docker compose logs -f             # Follow all service logs
```

For detailed setup and configuration, see individual service documentation in the `docs/` directory.
